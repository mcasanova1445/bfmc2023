{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "IMG_SIZE = (480, 640)\n",
    "SIZE = (32,32)\n",
    "ROI = [int(IMG_SIZE[0]/4), int(IMG_SIZE[0]*3/4), \n",
    "    int((IMG_SIZE[1]-IMG_SIZE[0]/2)/2), int(IMG_SIZE[1]-(IMG_SIZE[1]-IMG_SIZE[0]/2)/2)] #[a,b,c,d] ==> [a:b, c:d]\n",
    "model_name = 'models/pedestrian_classifier_small.pt'\n",
    "onnx_path = \"models/pedestrian_classifier_small.onnx\"\n",
    "max_load = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load examples\n",
    "examples_folder = 'pedestrian_roadblock_imgs'     \n",
    "names = [f for f in os.listdir(examples_folder) if f.endswith('.png')]\n",
    "classes = ['pedestrian','roadblock', 'free_road']\n",
    "example_imgs = [cv.imread(os.path.join(examples_folder, name), cv.IMREAD_GRAYSCALE) for name in names]\n",
    "example_labels = [0 if name.startswith('pedestrian') else 1 if name.startswith('roadblock') else 2 for name in names]\n",
    "tot_examples = len(example_imgs)\n",
    "tot_labels = len(classes)       \n",
    "\n",
    "#show images\n",
    "cv.namedWindow('example', cv.WINDOW_NORMAL)\n",
    "for i in range(tot_examples):\n",
    "    cv.imshow('example', example_imgs[i])\n",
    "    cv.waitKey(1)\n",
    "cv.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class FrontalClassifier(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 32, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=2), #out = 12\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), #out = 11\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2), #out = 9\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2), #out = 4\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out = 3\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=256, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "classifier = FrontalClassifier(out_dim=tot_labels,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = classifier(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "def load_and_augment_img(i, folder='training_imgs', example_index=0, example_imgs=example_imgs):\n",
    "    img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "    #convert to gray\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    # img = randint(0,255,(480,640), dtype=np.uint8)\n",
    "\n",
    "    #crop in the example ROI\n",
    "    img = img[ROI[0]:ROI[1], ROI[2]:ROI[3]]\n",
    "\n",
    "    #add random shapes to the image\n",
    "    num_vertices = 5\n",
    "    num_shapes = 3\n",
    "    for i in range(num_shapes):\n",
    "        #add polygon\n",
    "        ptsx = randint(0,img.shape[1],(num_vertices,))\n",
    "        ptsy = randint(0,img.shape[0],(num_vertices,))\n",
    "        pts = np.stack((ptsx,ptsy), axis=1)\n",
    "        img = cv.polylines(img, [pts], isClosed=False, color=randint(0,255), thickness=randint(2,10))\n",
    "        img = cv.circle(img, (randint(0,img.shape[1]), randint(0,img.shape[0])), randint(5,60), randint(0,255), randint(1,8))\n",
    "\n",
    "    ## EXAMPLE AUGMENTATION\n",
    "    #load example\n",
    "    example = example_imgs[example_index]\n",
    "    resize_ratio = 240./max(example.shape[0], example.shape[1])\n",
    "    example = cv.resize(example, (int(example.shape[1]*resize_ratio), int(example.shape[0]*resize_ratio)))\n",
    "    if np.random.uniform() < 0.5:\n",
    "        #flip\n",
    "        example = cv.flip(example, 1)\n",
    "\n",
    "\n",
    "    #get example mask\n",
    "    example_mask = np.where(example == 0, np.zeros_like(example), 255*np.ones_like(example))\n",
    "    #blur the example\n",
    "    example = cv.blur(example, (randint(3,9),randint(3,9)))\n",
    "\n",
    "    #add noise to the example\n",
    "    std = 100\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.subtract(example, noisem)\n",
    "    noisep = randint(0, std, example.shape, dtype=np.uint8)\n",
    "    example = cv.add(example, noisep)\n",
    "\n",
    "    #dilate\n",
    "    kernel = np.ones((randint(3,7),randint(3,7)), np.uint8)\n",
    "    example = cv.dilate(example, kernel, iterations=1)\n",
    "\n",
    "    #set zero where example mask is zero\n",
    "    example = np.where(example_mask == 0, np.zeros_like(example), example)\n",
    "    # cv.imshow('test', example)\n",
    "\n",
    "    #random rotation\n",
    "    angle = randint(-10,10)\n",
    "    M = cv.getRotationMatrix2D((example.shape[1]/2, example.shape[0]/2), angle, 1)\n",
    "    example = cv.warpAffine(example, M, (example.shape[1], example.shape[0]))\n",
    "\n",
    "    #perspective transform example\n",
    "    perspective_deformation = 30\n",
    "    pts1 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = np.float32([[0,0],[example.shape[1],0],[example.shape[1],example.shape[0]],[0,example.shape[0]]])\n",
    "    pts2 = pts2 + np.float32(randint(0,perspective_deformation,size=pts2.shape))\n",
    "    # print(f'pts2 = \\n{pts2}')\n",
    "    new_size_x = int(np.max(pts2[:,0]) - np.min(pts2[:,0]))\n",
    "    new_size_y = int(np.max(pts2[:,1]) - np.min(pts2[:,1]))\n",
    "    M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "    example = cv.warpPerspective(example,M,(new_size_x,new_size_y))\n",
    "\n",
    "    #resize example keeping proportions\n",
    "    img_example_ratio = min(img.shape[0]/example.shape[0], img.shape[1]/example.shape[1])\n",
    "    scale_factor = np.random.uniform(.75, .95)\n",
    "    scale_factor = scale_factor * img_example_ratio\n",
    "    example = cv.resize(example, (0,0), fx=scale_factor, fy=scale_factor)\n",
    "    #match img shape\n",
    "    example_canvas = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
    "\n",
    "    #get a random position for the example\n",
    "    example_y = randint(0, img.shape[0] - example.shape[0])\n",
    "    example_x = randint(0, img.shape[1] - example.shape[1])\n",
    "    #paste example on canvas\n",
    "    example_canvas[example_y:example_y+example.shape[0], example_x:example_x+example.shape[1]] = example\n",
    "\n",
    "    #reduce contrast\n",
    "    old_example_canvas = example_canvas.copy()\n",
    "    const = np.random.uniform(0.4,1.5)\n",
    "    example_canvas = 127*(1-const) + example_canvas*const\n",
    "    #clip values\n",
    "    example_canvas = np.clip(example_canvas, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #paste canvas on img\n",
    "    img = np.where(old_example_canvas > 0, example_canvas, img) \n",
    "    example_canvas = cv.blur(example_canvas, (randint(1,7),randint(1,7)))\n",
    "\n",
    "    # #create random ellipses to simulate light from the sun\n",
    "    # light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    # #add ellipses\n",
    "    # for j in range(2):\n",
    "    #     cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "    #     # axes_length = (randint(10, 50), randint(50, 300))\n",
    "    #     axes_length = (randint(1, 50), randint(10, 100))\n",
    "    #     angle = randint(0, 360)\n",
    "    #     light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    # #create an image of random white and black pixels\n",
    "    # noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    # light = cv.blur(light, (10,10))\n",
    "    # light = cv.subtract(light, noise)\n",
    "    # light = 1 * light\n",
    "    # #add light to the image\n",
    "    # img = cv.add(img, light)\n",
    "\n",
    "    #add noise \n",
    "    std = 80\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "    \n",
    "    #blur \n",
    "    img = cv.blur(img, (5,5))\n",
    "\n",
    "    #add random brightness\n",
    "    max_brightness = 50\n",
    "    brightness = randint(-max_brightness, max_brightness)\n",
    "    if brightness > 0:\n",
    "        img = cv.add(img, brightness)\n",
    "    elif brightness < 0:\n",
    "        img = cv.subtract(img, -brightness)\n",
    "    \n",
    "    # invert color\n",
    "    if np.random.uniform(0, 1) > 0.6:\n",
    "        img = cv.bitwise_not(img)\n",
    "\n",
    "    # resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # img = cv.equalizeHist(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "cv.namedWindow('test', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "example_index = 0\n",
    "for i in range(500):\n",
    "    img = load_and_augment_img(i, example_index=example_index)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "    example_index = (example_index + 1) % tot_labels\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, example_imgs=example_imgs, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "    \n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            \n",
    "            example_index = 0\n",
    "\n",
    "            for i in tqdm(range(max_load)):\n",
    "                #img \n",
    "                img = load_and_augment_img(i, example_index=example_index, example_imgs=example_imgs)\n",
    "\n",
    "                max_show = 500\n",
    "                if i < max_show:\n",
    "                    cv.imshow('img', img)\n",
    "                    cv.waitKey(1)\n",
    "                    if i == (max_show-1):\n",
    "                        cv.destroyAllWindows()\n",
    "                \n",
    "                #add a dimension to the image\n",
    "                img = img[:, :,np.newaxis]\n",
    "                self.all_imgs[i] = torch.from_numpy(img)\n",
    "                \n",
    "                #label (it is just the index of the example)\n",
    "                self.data.append(example_index)  \n",
    "                example_index = (example_index + 1) % tot_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:33<00:00, 106.92it/s]\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 32, 32])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)\n",
    "# cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# for i in range(100):\n",
    "#     img = sample[0][i].permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "#     label = sample[1][i]\n",
    "#     # print(img.shape)\n",
    "#     cv.imshow(names[label], img)\n",
    "#     k = cv.waitKey(0)\n",
    "#     cv.destroyAllWindows()\n",
    "#     if k == ord('q') or k == 27:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, loss_fn, optimizer, device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, label) in tqdm(dataloader):\n",
    "        #convert label in one-hot vector\n",
    "        target = torch.eye(tot_labels)[label] \n",
    "        # Move the input and target data to the selected device\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #Loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    loss = np.mean(losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "loss: 0.576439380645752\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classifier.to(device)\n",
    "#parameters\n",
    "lr = 0.001\n",
    "epochs = 1\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=lr, weight_decay=3e-5) #3e-5\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(epochs):\n",
    "    # try:\n",
    "    if True:\n",
    "        loss = train_epoch(classifier, train_dataloader, loss_fn, optimizer, device)\n",
    "        clear_output(wait=True)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     continue\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"loss: {loss}\")\n",
    "    torch.save(classifier.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "classifier.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "classifier.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(classifier, dummy_input, onnx_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/pedestrian_classifier_small.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -0.73728687  16.127693   -18.813717  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:02<04:42,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.31071287  4.08805    -5.1593347 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:03<02:19,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.3980618  3.746822  -4.9570856]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:03<01:32,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -0.39833432   9.148591   -10.418233  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:04<01:13,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [  1.1312968  10.200394  -13.341937 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:04<01:03,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.7235406  7.239696  -9.677542 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:05<00:57,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -0.6212154  11.732239  -12.972123 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:05<00:54,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.2804069  7.496891  -9.387354 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:06<00:38,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -1.8555509  18.21523   -20.06373  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:06<00:32,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -1.874459  16.459103 -17.096992]\n",
      "preds: [ 5.458214  -6.355572  -1.0987706]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:07<00:36,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 1.4117874   0.65696704 -2.6261353 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:07<00:39,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 4.6434746  -5.941686   -0.46274725]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:08<00:42,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -0.1306749  12.566153  -15.057537 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:08<00:44,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 2.0031974 -4.279322   1.5215364]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:09<00:56,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.01869263  4.7329354  -5.4449687 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:10<01:04,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.18916178  3.9323244  -4.8915963 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:11<00:56,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -0.3565766   9.06713   -10.486619 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:11<00:51,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -2.3537576  21.346931  -22.837492 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:12<00:48,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.40724927 -4.239979    3.4637053 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:12<00:47,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.65423924  2.0089638  -3.186534  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:13<00:46,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -1.6031246  16.104263  -17.48974  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:13<00:42,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 3.3196917 -5.517394   0.8022279]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:14<00:42,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 2.010482   -2.0907686  -0.71604085]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:14<00:40,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 2.742787  -8.065161   3.7410886]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:15<00:27,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -1.2064304  18.261969  -20.73418  ]\n",
      "preds: [ 0.67057693 -4.6458693   3.59709   ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:15<00:20,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 3.3368814 -1.123427  -3.2434156]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:15<00:18,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 4.3585916 -6.833519   0.6875418]\n",
      "preds: [ 0.7102581  0.9531143 -1.9959917]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:16<00:23,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -0.22829854  13.24263    -15.976512  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:17<00:20,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 1.2023199 -6.5594196  4.4936223]\n",
      "preds: [ -0.88628   15.305637 -17.364729]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:17<00:20,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 5.1317844 -5.776065  -1.2257028]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [00:17<00:19,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 1.9619266 -1.2709882 -1.438403 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:17<00:18,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 5.0312877 -3.9410217 -2.87509  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:18<00:16,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 2.4220166  -4.527466    0.97134197]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:18<00:17,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -2.0804355  22.40704   -24.695654 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [00:18<00:16,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.28084692 -4.1957383   3.5304706 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:19<00:14,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -0.8658131  14.786174  -16.816664 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:19<00:13,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 1.288798 -6.686943  4.483503]\n",
      "preds: [ 0.53266054  3.9168434  -5.1586533 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:19<00:15,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.38160944  3.8491364  -5.0349455 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:20<00:18,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 2.2057195 -7.2906294  3.7905917]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:20<00:20,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -1.4550737  15.366939  -16.92294  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:21<00:22,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -1.3947803  13.127969  -14.260843 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:21<00:22,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -0.42924935  11.928717   -14.278844  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:21<00:22,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.55028826  1.9768213  -2.8125806 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [00:22<00:24,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.47401297  1.8027357  -2.6341093 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:23<00:23,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.57706326 -1.2674538   0.5608396 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:23<00:23,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -0.59004337  14.169845   -16.455698  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:24<00:25,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 1.134756  -8.212007   5.9962587]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:24<00:26,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 2.1549017  6.275324  -9.778039 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:25<00:24,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ -1.6806527  20.863087  -23.280863 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [00:25<00:22,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [ 0.735347   -0.5480509  -0.32499397]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [00:35<00:30,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 0\n",
      "Predictions shape: ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test with opencv\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_path) \n",
    "print(onnx_path)\n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "sample = next(iter(train_dataloader))\n",
    "images = [sample[0][i].permute(1, 2, 0).numpy().astype(np.uint8) for i in range(100)]\n",
    "labels = [sample[1][i] for i in range(100)]\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    #add noise \n",
    "    std = 80\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, image.shape, dtype=np.uint8)\n",
    "    image = cv.subtract(image, noisem)\n",
    "    noisep = randint(0, std, image.shape, dtype=np.uint8)\n",
    "    image = cv.add(image, noisep)\n",
    "    image = cv.blur(image, (3,3))\n",
    "    # if num_channels == 1:\n",
    "    #     image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    assert SIZE == (32, 32)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    preds = preds[0]\n",
    "    print(f'preds: {preds}')\n",
    "    preds = preds.argmax()\n",
    "    cv.putText(image, names[preds], (5, 10), cv.FONT_HERSHEY_SIMPLEX, 0.3, 0, 1)\n",
    "    #put true label\n",
    "    cv.putText(image, names[labels[i]], (5, 20), cv.FONT_HERSHEY_SIMPLEX, 0.3, 0, 1)\n",
    "    cv.imshow(\"img\", image)\n",
    "    k = cv.waitKey(0)   \n",
    "    if k == ord('q') or k == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get image and label\n",
    "# cv.namedWindow('img')\n",
    "# dataloader = DataLoader(train_dataset, batch_size=10000, shuffle=False)\n",
    "# for i, (imgs, labels) in enumerate(tqdm(dataloader)):\n",
    "#     #convert img to numpy\n",
    "#     imgs = imgs.cpu().numpy()\n",
    "#     for i in range(imgs.shape[0]):\n",
    "#         img = imgs[i][0]\n",
    "#         #convert to uint8d\n",
    "#         img = img.astype(np.uint8)\n",
    "#         cv.imshow(\"img\", img)\n",
    "#         cv.waitKey(1)\n",
    "\n",
    "# cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
